{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk, re\n",
    "import pandas as pd\n",
    "import sparknlp\n",
    "import os\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark import SQLContext\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql import functions as fn\n",
    "import time\n",
    "from pyspark.sql.functions import udf, to_date, date_format\n",
    "from pyspark.sql.types import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"sentiment_analysis\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "def clean(x):\n",
    "#      remove the retweeted tweet\n",
    "     x=str(x)\n",
    "     x = re.sub(r'^RT[\\s]+','',x)\n",
    "#      Replace #word with word\n",
    "     x = re.sub(r'#', '', x)\n",
    "#      Convert @username to empty strings\n",
    "     x= re.sub('@[^\\s]+', '', x) \n",
    "#       Convert www.* or https?://* to empty strings\n",
    "     x= re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '',x)\n",
    "#     Remove all characters which are not alphabets, numbers or whitespaces.\n",
    "     x = re.sub('[^A-Za-z0-9 ]+','', x)\n",
    "     return x\n",
    "udfclean=udf(clean,StringType())\n",
    "\n",
    "def lemmatizer(tweet):\n",
    "    word_list = []\n",
    "    for word in tweet.split():\n",
    "        word_list.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return (\" \".join(word_list))    \n",
    "udflemmatizer=udf(lemmatizer,StringType())\n",
    "def sample(x):\n",
    "    return 5000/x\n",
    "udfsample=udf(sample,FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------------+\n",
      "|      time|           user|                text|\n",
      "+----------+---------------+--------------------+\n",
      "|2020-10-16|        sjrlady|@abc are you goin...|\n",
      "|2020-10-16|       jayjer24|what's up @realdo...|\n",
      "|2020-10-16|    SheilaYehle|last night in eas...|\n",
      "|2020-10-16|    geurtje1914|@realdonaldtrump ...|\n",
      "|2020-10-16|KyleCoo32421721|#crookedjoebiden ...|\n",
      "|2020-10-16|DIDNOTVOTEFOR44|president trump w...|\n",
      "|2020-10-16|    lamantide71|@realdonaldtrump ...|\n",
      "|2020-10-16|  ElianaBenador|is it #sleepyjoe ...|\n",
      "|2020-10-16| alexander12ray|@realdonaldtrump ...|\n",
      "|2020-10-16|  RealCaryPrice|https://t.co/iybd...|\n",
      "+----------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trump_tweet=sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('trump.csv')\n",
    "trump_tweet=trump_tweet.dropna()\n",
    "trump_tweet=trump_tweet.distinct()\n",
    "trump=trump_tweet.filter(fn.col(\"time\").isin(['2020-10-15','2020-10-16','2020-10-17','2020-10-18','2020-10-19','2020-10-20','2020-10-21','2020-10-22','2020-10-23',\n",
    "                                             '2020-10-24','2020-10-25','2020-10-26','2020-10-27','2020-10-28','2020-10-29','2020-10-30','2020-10-31','2020-11-01','2020-11-02','2020-11-03']))\n",
    "trump.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+\n",
      "|      time|count|     fract|\n",
      "+----------+-----+----------+\n",
      "|2020-10-15| 8851| 0.5649079|\n",
      "|2020-10-16| 6995| 0.7147963|\n",
      "|2020-10-17| 9476| 0.5276488|\n",
      "|2020-10-18| 9686|0.51620895|\n",
      "|2020-10-19|12542| 0.3986605|\n",
      "|2020-10-20|14592| 0.3426535|\n",
      "|2020-10-21|10420|0.47984645|\n",
      "|2020-10-22|35188| 0.1420939|\n",
      "|2020-10-23|13313|0.37557274|\n",
      "|2020-10-24|17364| 0.2879521|\n",
      "|2020-10-25| 9782| 0.5111429|\n",
      "|2020-10-26|31814|0.15716352|\n",
      "|2020-10-27|15701|0.31845105|\n",
      "|2020-10-28|11031|0.45326805|\n",
      "|2020-10-29| 8254| 0.6057669|\n",
      "|2020-10-30|12390|0.40355125|\n",
      "|2020-10-31|12567|0.39786744|\n",
      "|2020-11-01| 7925| 0.6309148|\n",
      "|2020-11-02|13036|0.38355324|\n",
      "|2020-11-03|13115|0.38124284|\n",
      "+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count=trump.groupBy('time').agg(fn.count('*').alias('count')).orderBy(trump.time.asc())\n",
    "count=count.withColumn('fract',udfsample('count'))\n",
    "count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2020-10-15': 0.5649079084396362, '2020-10-16': 0.7147963047027588, '2020-10-17': 0.5276488065719604, '2020-10-18': 0.5162089467048645, '2020-10-19': 0.3986605107784271, '2020-10-20': 0.3426535129547119, '2020-10-21': 0.47984644770622253, '2020-10-22': 0.14209389686584473, '2020-10-23': 0.37557274103164673, '2020-10-24': 0.28795209527015686, '2020-10-25': 0.511142909526825, '2020-10-26': 0.15716351568698883, '2020-10-27': 0.31845104694366455, '2020-10-28': 0.45326805114746094, '2020-10-29': 0.6057668924331665, '2020-10-30': 0.40355125069618225, '2020-10-31': 0.39786744117736816, '2020-11-01': 0.6309148073196411, '2020-11-02': 0.38355323672294617, '2020-11-03': 0.3812428414821625}\n"
     ]
    }
   ],
   "source": [
    "fractions = count.select(\"time\",'fract').rdd.collectAsMap()\n",
    "print(fractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump=trump.sampleBy('time',fractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100159\n",
      "+----------+---------------+--------------------+\n",
      "|      time|           user|                text|\n",
      "+----------+---------------+--------------------+\n",
      "|2020-10-16|        sjrlady|@abc are you goin...|\n",
      "|2020-10-16|       jayjer24|what's up @realdo...|\n",
      "|2020-10-16|    SheilaYehle|last night in eas...|\n",
      "|2020-10-16|    geurtje1914|@realdonaldtrump ...|\n",
      "|2020-10-16|DIDNOTVOTEFOR44|president trump w...|\n",
      "|2020-10-16|    lamantide71|@realdonaldtrump ...|\n",
      "|2020-10-16|  ElianaBenador|is it #sleepyjoe ...|\n",
      "|2020-10-16| alexander12ray|@realdonaldtrump ...|\n",
      "|2020-10-16|  RealCaryPrice|https://t.co/iybd...|\n",
      "|2020-10-16|       jim62192|when have you eve...|\n",
      "|2020-10-16|DaleFer99092112|@realdonaldtrump ...|\n",
      "|2020-10-16|   jmmtcarvalho|looking good. #st...|\n",
      "|2020-10-16|        jos1963|@dickberlijn dick...|\n",
      "|2020-10-16|     BrownFayeM|kinda what hillar...|\n",
      "|2020-10-16|EmmanuelKatoto1|@realdonaldtrump ...|\n",
      "|2020-10-16|  LuanneAWilson|@realdonaldtrump ...|\n",
      "|2020-10-16|   TheChosenOn8|although the timi...|\n",
      "|2020-10-16|     Akkara7771|@realdonaldtrump ...|\n",
      "|2020-10-16| King1318341606|@realdonaldtrump ...|\n",
      "|2020-10-17| davidaarongrig|@realdonaldtrump ...|\n",
      "+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(trump.count())\n",
    "trump.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "trump=trump.withColumn('clean_words',udfclean('text'))\n",
    "trump=trump.withColumn('newText',udflemmatizer('clean_words'))\n",
    "#trump=trump.withColumn('clean_words',udfclean('text'))\n",
    "trump=trump.select('newText')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [u'rt',u're', u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your',\n",
    "             u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers',\n",
    "             u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what',\n",
    "             u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were',\n",
    "             u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a',\n",
    "             u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by',\n",
    "             u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after',\n",
    "             u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under',\n",
    "             u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all',\n",
    "             u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not',\n",
    "             u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don',\n",
    "             u'should', u'now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The preopocessing time : 0.16986775398254395\n",
      "The tf-idf time : 0.03187274932861328\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             newText|               words|            filtered|                  tf|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| are you going to...|[, are, you, goin...|[, going, factche...|(65536,[9191,1548...|(65536,[9191,1548...|\n",
      "|           whats up |         [whats, up]|             [whats]|(65536,[48531],[1...|(65536,[48531],[5...|\n",
      "|last night in eas...|[last, night, in,...|[last, night, eas...|(65536,[4166,5381...|(65536,[4166,5381...|\n",
      "| theres only one ...|[, theres, only, ...|[, theres, one, a...|(65536,[16499,218...|(65536,[16499,218...|\n",
      "|president trump w...|[president, trump...|[president, trump...|(65536,[14524,221...|(65536,[14524,221...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"newText\",outputCol='words')\n",
    "sw_filter = StopWordsRemover()\\\n",
    "  .setStopWords(stopwords)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered\")\n",
    "#trump=trump.select('filtered')\n",
    "end_time=time.time()\n",
    "print('The preopocessing time :',end_time-start_time)\n",
    "#countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\",\n",
    "#vocabSize=10000, minDF=5)\n",
    "start_time=time.time()\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"filtered\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "end_time=time.time()\n",
    "print('The tf-idf time :',end_time-start_time)\n",
    "#label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer,sw_filter, hashtf,idf])\n",
    "pipelineFit = pipeline.fit(trump)\n",
    "trump = pipelineFit.transform(trump)\n",
    "trump.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment=sqlContext.read.format('com.databricks.spark.csv').options(header='True', inferschema='true',sep='\\t').load('AFINN.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|     word|sentiment|\n",
      "+---------+---------+\n",
      "|abandoned|       -2|\n",
      "| abandons|       -2|\n",
      "+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- sentiment: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|     word|sentiment|\n",
      "+---------+---------+\n",
      "|   aboard|        1|\n",
      "| absorbed|        1|\n",
      "|   accept|        1|\n",
      "| accepted|        1|\n",
      "|accepting|        1|\n",
      "+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment.where(fn.col('sentiment') == 1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|     word|sentiment|\n",
      "+---------+---------+\n",
      "| absentee|       -1|\n",
      "|absentees|       -1|\n",
      "|    admit|       -1|\n",
      "|   admits|       -1|\n",
      "| admitted|       -1|\n",
      "+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment.where(fn.col('sentiment') == -1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|       -5|   16|\n",
      "|       -4|   43|\n",
      "|       -3|  264|\n",
      "|       -2|  965|\n",
      "|       -1|  309|\n",
      "|        0|    1|\n",
      "|        1|  208|\n",
      "|        2|  448|\n",
      "|        3|  172|\n",
      "|        4|   45|\n",
      "|        5|    5|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment.groupBy('sentiment').agg(fn.count('*').alias('count')).orderBy(sentiment.sentiment.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            filtered|                word|\n",
      "+--------------------+--------------------+\n",
      "|[, going, factche...|                    |\n",
      "|[, going, factche...|               going|\n",
      "|[, going, factche...|           factcheck|\n",
      "|[, going, factche...|               biden|\n",
      "|[, going, factche...|                    |\n",
      "|[, going, factche...|                 lie|\n",
      "|[, going, factche...|georgestephanopoulos|\n",
      "|[, going, factche...|           townhalls|\n",
      "|             [whats]|               whats|\n",
      "|[last, night, eas...|                last|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trump.select('filtered', fn.explode('filtered').alias('word')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+\n",
      "|      word|            filtered|sentiment|\n",
      "+----------+--------------------+---------+\n",
      "|     great|[last, night, eas...|        3|\n",
      "|       yes|[last, night, eas...|        1|\n",
      "|      hell|[last, night, eas...|       -4|\n",
      "|   loyalty|         [, loyalty]|        3|\n",
      "|disrespect|[ever, witnessed,...|       -2|\n",
      "|      good|[looking, good, s...|        3|\n",
      "|      dick|[, dick, berlijn,...|       -4|\n",
      "|      like|[, like, governan...|        2|\n",
      "| important|[, like, governan...|        2|\n",
      "|    matter|[, like, governan...|        1|\n",
      "+----------+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "review_word_sentiment_df = trump.\\\n",
    "    select('filtered', fn.explode('filtered').alias('word')).\\\n",
    "    join(sentiment, 'word')\n",
    "review_word_sentiment_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment analysis time : 1.834254503250122\n",
      "+--------------------+-------------+\n",
      "|            filtered|avg_sentiment|\n",
      "+--------------------+-------------+\n",
      "|           [, thank]|          2.0|\n",
      "|[yes, believe, je...|          1.0|\n",
      "|[idiot, youre, pr...|         -3.0|\n",
      "|[sad, happen, nei...|          0.0|\n",
      "|[, introduction, ...|          3.0|\n",
      "|    [, talking, ass]|         -4.0|\n",
      "|[, broken, record...|          0.0|\n",
      "|[, pcr, tests, de...|         -3.0|\n",
      "|[, justice, amy, ...|          2.0|\n",
      "|[, youve, lost, b...|         -3.0|\n",
      "|[, date, us, high...|         -2.0|\n",
      "|[, alzheimers, ki...|          2.0|\n",
      "|[, another, 100, ...|         -3.0|\n",
      "|       [, get, lost]|         -3.0|\n",
      "|[waitso, , likes,...|          2.0|\n",
      "|[, list, top, int...|          0.0|\n",
      "|[, cant, air, 60,...|         -2.5|\n",
      "|[yo, prez, , fit,...|         -1.0|\n",
      "|[, hat, made, goo...|          3.0|\n",
      "|   [charged, crimes]|         -3.0|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "simple_sentiment_prediction_df = review_word_sentiment_df.\\\n",
    "    groupBy('filtered').\\\n",
    "    agg(fn.avg('sentiment').alias('avg_sentiment'))\n",
    "end_time=time.time()\n",
    "print('sentiment analysis time :', end_time-start_time)\n",
    "simple_sentiment_prediction_df.show(20)\n",
    "#simple_sentiment_prediction_df= simple_sentiment_prediction_df.withColumn(\"avg_sentiment\",simple_sentiment_prediction_df[\"avg_sentiment\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive:  0.44480273551626376\n",
      "negative:  0.5087540848789145\n",
      "neutral:  0.04644317960482178\n"
     ]
    }
   ],
   "source": [
    "positive=simple_sentiment_prediction_df.filter('avg_sentiment>0').count()\n",
    "negative=simple_sentiment_prediction_df.filter('avg_sentiment<0').count()\n",
    "neutral=simple_sentiment_prediction_df.filter('avg_sentiment==0').count()\n",
    "total=simple_sentiment_prediction_df.count()\n",
    "p=positive/total\n",
    "n=negative/total\n",
    "neutral=neutral/total\n",
    "print('positive: ',p)\n",
    "print('negative: ', n)\n",
    "print('neutral: ',neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------------+\n",
      "|      time|           user|                 text|\n",
      "+----------+---------------+---------------------+\n",
      "|2020-10-16|        sjrlady| @abc are you goin...|\n",
      "|2020-10-16| SuzieQ74731364| #crookedjoebiden ...|\n",
      "|2020-10-16|    BobGeloneck| @joebiden these w...|\n",
      "|2020-10-16|      bezivonne|@joebiden 鈼?bengh...|\n",
      "|2020-10-16|BabaGan15001591| @joebiden did hun...|\n",
      "|2020-10-16|        4Shaner| this was supposed...|\n",
      "|2020-10-16|  ElianaBenador| is it #sleepyjoe ...|\n",
      "|2020-10-16|  NancyLChapman| @joebiden you ain...|\n",
      "|2020-10-16|   MiguelAmorim| \"@joebiden \"\"my d...|\n",
      "|2020-10-16|   BellaRisttaa| @joebiden https:/...|\n",
      "+----------+---------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "{'2020-10-15': 0.48680752515792847, '2020-10-16': 0.5899705290794373, '2020-10-17': 0.6924248933792114, '2020-10-18': 0.7588405013084412, '2020-10-19': 0.6368615627288818, '2020-10-20': 0.37605294585227966, '2020-10-21': 0.6747638583183289, '2020-10-22': 0.4411894381046295, '2020-10-23': 0.5785028338432312, '2020-10-24': 0.5356186628341675, '2020-10-25': 0.5346450209617615, '2020-10-26': 0.5150921940803528, '2020-10-27': 0.5864414572715759, '2020-10-28': 0.5468066334724426, '2020-10-29': 0.5822755098342896, '2020-10-30': 0.5462093353271484, '2020-10-31': 0.6379992365837097, '2020-11-01': 0.5306729078292847, '2020-11-02': 0.5391998291015625, '2020-11-03': 0.6065016984939575}\n"
     ]
    }
   ],
   "source": [
    "biden_tweet=sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('biden.csv')\n",
    "biden_tweet=biden_tweet.dropna()\n",
    "biden_tweet=biden_tweet.distinct()\n",
    "biden=biden_tweet.filter(fn.col(\"time\").isin(['2020-10-15','2020-10-16','2020-10-17','2020-10-18','2020-10-19','2020-10-20','2020-10-21','2020-10-22','2020-10-23',\n",
    "                                             '2020-10-24','2020-10-25','2020-10-26','2020-10-27','2020-10-28','2020-10-29','2020-10-30','2020-10-31','2020-11-01','2020-11-02','2020-11-03']))\n",
    "biden.show(10)\n",
    "count=biden.groupBy('time').agg(fn.count('*').alias('count')).orderBy(biden.time.asc())\n",
    "count=count.withColumn('fract',udfsample('count'))\n",
    "fractions = count.select(\"time\",'fract').rdd.collectAsMap()\n",
    "print(fractions)\n",
    "biden=biden.sampleBy('time',fractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99861\n",
      "+----------+---------------+---------------------+\n",
      "|      time|           user|                 text|\n",
      "+----------+---------------+---------------------+\n",
      "|2020-10-16|        sjrlady| @abc are you goin...|\n",
      "|2020-10-16| SuzieQ74731364| #crookedjoebiden ...|\n",
      "|2020-10-16|      bezivonne|@joebiden 鈼?bengh...|\n",
      "|2020-10-16|BabaGan15001591| @joebiden did hun...|\n",
      "|2020-10-16|  ElianaBenador| is it #sleepyjoe ...|\n",
      "|2020-10-16|  NancyLChapman| @joebiden you ain...|\n",
      "|2020-10-16|   BellaRisttaa| @joebiden https:/...|\n",
      "|2020-10-16| Forest_StarIII| @joebiden vote #c...|\n",
      "|2020-10-16|   JupiterWalls| nothing about thi...|\n",
      "|2020-10-17|DanielBolling14| @joebiden #crooke...|\n",
      "|2020-10-17|     GoonaJames| he's taking to li...|\n",
      "|2020-10-17|1perplepassion8|     #bidenharris2020|\n",
      "|2020-10-17|       ForgeRat|    #chickentrump馃悢|\n",
      "|2020-10-17|       SMohitR1| aisa hai tu india...|\n",
      "|2020-10-17|   3toedgiraffe| @joebiden still i...|\n",
      "|2020-10-17|       Werdgerl|     @realdonaldtrump|\n",
      "|2020-10-17|       JSolomos| @joebiden keyword...|\n",
      "|2020-10-17| JakeMHenderson|            a leader!|\n",
      "|2020-10-17| notmrsrobinson| @joebiden serious...|\n",
      "|2020-10-17| susie_margaret| @joebiden make no...|\n",
      "+----------+---------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(biden.count())\n",
    "biden.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "biden=biden.withColumn('clean_words',udfclean('text'))\n",
    "biden=biden.withColumn('newText',udflemmatizer('clean_words'))\n",
    "biden=biden.select('newText')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The preopocessing time : 0.0638282299041748\n",
      "tf-idf time :  0.005985260009765625\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             newText|               words|            filtered|                  tf|                 idf|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| are you going to...|[, are, you, goin...|[, going, factche...|(65536,[9191,1548...|(65536,[9191,1548...|\n",
      "|    crookedjoebiden |   [crookedjoebiden]|   [crookedjoebiden]|(65536,[57900],[1...|(65536,[57900],[5...|\n",
      "| benghazi whistle...|[, benghazi, whis...|[, benghazi, whis...|(65536,[3498,1630...|(65536,[3498,1630...|\n",
      "| did hunter have ...|[, did, hunter, h...|[, hunter, pay, w...|(65536,[43105,472...|(65536,[43105,472...|\n",
      "|is it sleepyjoe o...|[is, it, sleepyjo...|[sleepyjoe, lousy...|(65536,[8605,2617...|(65536,[8605,2617...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "tokenizer = Tokenizer(inputCol=\"newText\",outputCol='words')\n",
    "sw_filter = StopWordsRemover()\\\n",
    "  .setStopWords(stopwords)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered\")\n",
    "end_time=time.time()\n",
    "print('The preopocessing time :',end_time-start_time)\n",
    "#countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\",\n",
    "#vocabSize=10000, minDF=5)\n",
    "start_time=time.time()\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"filtered\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"idf\", minDocFreq=5)\n",
    "end_time=time.time()\n",
    "print('tf-idf time : ',end_time-start_time)\n",
    "#minDocFreq: remove sparse terms\n",
    "#label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer,sw_filter, hashtf,idf])\n",
    "pipelineFit = pipeline.fit(biden)\n",
    "biden = pipelineFit.transform(biden)\n",
    "biden.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------+\n",
      "|     word|            filtered|sentiment|\n",
      "+---------+--------------------+---------+\n",
      "|      pay|[, hunter, pay, w...|       -1|\n",
      "|incapable|[aisa, hai, tu, i...|       -2|\n",
      "|    shock|[, still, shock, ...|       -2|\n",
      "|  mistake|[, make, mistake,...|       -2|\n",
      "|     care|[, make, mistake,...|        2|\n",
      "|     fake|[corrupt, joe, bi...|       -3|\n",
      "|  fucking|[, ain, fucking, ...|       -4|\n",
      "|     fuck|[, ain, fucking, ...|       -4|\n",
      "|     want|[, wanted, shut, ...|        1|\n",
      "|     want|[nobody, ever, sa...|        1|\n",
      "+---------+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "review_word_sentiment_df2 = biden.\\\n",
    "    select('filtered', fn.explode('filtered').alias('word')).\\\n",
    "    join(sentiment, 'word')\n",
    "review_word_sentiment_df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment analysis time : 1.5774781703948975\n",
      "+--------------------+------------------+\n",
      "|            filtered|     avg_sentiment|\n",
      "+--------------------+------------------+\n",
      "|           [, thank]|               2.0|\n",
      "|[, kammy, joke, t...|               2.0|\n",
      "|[, thomas, engage...|              -1.5|\n",
      "|[, blah, blah, bl...|              -2.0|\n",
      "|[corruptjoebiden,...|              -0.5|\n",
      "|[, dont, forget, ...|              -1.0|\n",
      "|[, virus, came, c...|0.3333333333333333|\n",
      "|[, cool, demagogu...|               1.0|\n",
      "|       [, get, lost]|              -3.0|\n",
      "|[, thanks, lying,...|               2.0|\n",
      "|[, unfortunately,...|               0.0|\n",
      "|[, theyre, going,...|               2.5|\n",
      "|[ugly, people, tr...|              -3.0|\n",
      "|[, say, 100, year...|               2.0|\n",
      "|[biden, says, cat...|               1.5|\n",
      "|[, going, shut, v...|               1.5|\n",
      "|[, ridiculous, st...|              -1.0|\n",
      "|[, danny, glover,...|              -2.0|\n",
      "|     [, love, trump]|               3.0|\n",
      "|[, communist, par...|              -3.0|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simple_sentiment_prediction_df2 = review_word_sentiment_df2.\\\n",
    "    groupBy('filtered').\\\n",
    "    agg(fn.avg('sentiment').alias('avg_sentiment'))\n",
    "end_time=time.time()\n",
    "print('sentiment analysis time :', end_time-start_time)\n",
    "simple_sentiment_prediction_df2.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive:  0.5226106911447084\n",
      "negative:  0.43115550755939525\n",
      "neutral:  0.046233801295896326\n"
     ]
    }
   ],
   "source": [
    "positive=simple_sentiment_prediction_df2.filter('avg_sentiment>0').count()\n",
    "negative=simple_sentiment_prediction_df2.filter('avg_sentiment<0').count()\n",
    "neutral=simple_sentiment_prediction_df2.filter('avg_sentiment==0').count()\n",
    "total=simple_sentiment_prediction_df2.count()\n",
    "p=positive/total\n",
    "n=negative/total\n",
    "neutral=neutral/total\n",
    "print('positive: ',p)\n",
    "print('negative: ', n)\n",
    "print('neutral: ',neutral)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
